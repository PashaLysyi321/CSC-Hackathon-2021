{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"4nn 3task.ipynb\"\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PashaLysyi321/CSC-Hackathon-2021/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%224nn_3task_ipynb%22%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "5c400c6c-7afe-43d2-e779-0cb025d956fd"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 1.3MB/s eta 0:04:13tcmalloc: large alloc 1147494400 bytes == 0x5638ec43c000 @  0x7fe46c0da615 0x5638b2908cdc 0x5638b29e852a 0x5638b290bafd 0x5638b29fcfed 0x5638b297f988 0x5638b297a4ae 0x5638b290d3ea 0x5638b297f7f0 0x5638b297a4ae 0x5638b290d3ea 0x5638b297c32a 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b2a803e1 0x5638b29e06a9 0x5638b294bcc4 0x5638b290c559 0x5638b29804f8 0x5638b290d30a 0x5638b297b3b5 0x5638b297a7ad 0x5638b290d3ea 0x5638b297b3b5 0x5638b290d30a 0x5638b297b3b5\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.3MB/s eta 0:01:16tcmalloc: large alloc 1434370048 bytes == 0x563930a92000 @  0x7fe46c0da615 0x5638b2908cdc 0x5638b29e852a 0x5638b290bafd 0x5638b29fcfed 0x5638b297f988 0x5638b297a4ae 0x5638b290d3ea 0x5638b297f7f0 0x5638b297a4ae 0x5638b290d3ea 0x5638b297c32a 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b297b853 0x5638b29fde36 0x5638b2a803e1 0x5638b29e06a9 0x5638b294bcc4 0x5638b290c559 0x5638b29804f8 0x5638b290d30a 0x5638b297b3b5 0x5638b297a7ad 0x5638b290d3ea 0x5638b297b3b5 0x5638b290d30a 0x5638b297b3b5\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.4MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x56398627e000 @  0x7fe46c0da615 0x5638b2908cdc 0x5638b29e852a 0x5638b290bafd 0x5638b29fcfed 0x5638b297f988 0x5638b297a4ae 0x5638b290d3ea 0x5638b297b60e 0x5638b297a4ae 0x5638b290d3ea 0x5638b297b60e 0x5638b297a4ae 0x5638b290d3ea 0x5638b297b60e 0x5638b297a4ae 0x5638b290d3ea 0x5638b297b60e 0x5638b297a4ae 0x5638b290d3ea 0x5638b297b60e 0x5638b290d30a 0x5638b297b60e 0x5638b297a4ae 0x5638b290d3ea 0x5638b297c32a 0x5638b297a4ae 0x5638b290d3ea 0x5638b297c32a 0x5638b297a4ae 0x5638b290da81\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 16kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9MB 235kB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=a0a3b3de232e13fc6bb930031d9e99e56f2a30be6945d68e9c07cc11f020e0cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, ftfy\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed ftfy-6.0.3 torch-1.7.1+cu110 torchvision-0.8.2+cu110\n",
            "Torch version: 1.7.1+cu110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Загружаем CLIP\n",
        "\n",
        "Скачиваем CLIP, предобученный на 400М пар изображение-текст.  Его можно использовать в режиме обучения без обучения (например ViT-B/32 CLIP). После запуска блока нас ждет установка скачивание model.pt модели CLIP: Visual Transformer \"ViT-B/32\" + Text Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTJ3CrSIw2fe",
        "outputId": "3a60e595-21f4-4e44-a19a-e9ea6fd70c30"
      },
      "source": [
        "MODELS = {\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "}\n",
        "\n",
        "! wget {MODELS[\"RN50x4\"]} -O model.pt\n",
        "import numpy as np\n",
        "import torch\n",
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "\n",
        "\n",
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-04 12:29:54--  https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.40, 13.107.213.40, 2620:1ec:bdf::40, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 421854225 (402M) [application/octet-stream]\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>] 402.31M   115MB/s    in 3.5s    \n",
            "\n",
            "2021-07-04 12:29:58 (114 MB/s) - ‘model.pt’ saved [421854225/421854225]\n",
            "\n",
            "Model parameters: 178,300,601\n",
            "Input resolution: 288\n",
            "Context length: 77\n",
            "Vocab size: 49408\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "--2021-07-04 12:30:06--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.40, 13.107.213.40, 2620:1ec:bdf::40, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-07-04 12:30:06 (23.5 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Препроцессинг Текста\n",
        "\n",
        "Текстовый препроцессинг для Text Transformer части сети CLIP использует  без нечувствительный к регистру токенизатор. Код токенизатора скрыт во второй ячейке блока. Далее текст паддится до длины сontext length, и готов подаваться в трансформер."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Подготовка к обучению без обучения\n",
        "\n",
        "Перед непосредственным обучением без обучения, давайте разберем на примере, как работает CLIP. \n",
        "\n",
        "Давайте  скормим модели 10 изображений по одному примеру на класс и их текстовые описания. А потом построим матрицу косинусных расстояний между векторами изображений и векторами текстов (cosine similarity в общем пространстве визуальных и текстовых репрезентаций). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrBE3hCfJhU8"
      },
      "source": [
        "### Ссылки на изображения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGM7nGE0RvmM"
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-erzoVgzoT4"
      },
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import os\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import shutil\n",
        "\n",
        "def main_processing(classes):\n",
        "  images = []\n",
        "  one_per_class = []\n",
        "\n",
        "  per_class_lists = []\n",
        "\n",
        "  list_sorted_of_photo = sorted(os.listdir(path=\"/content/input_images/\"))\n",
        "  for i in list_sorted_of_photo:\n",
        "    im = Image.open(\"/content/input_images/\"+i)\n",
        "    image = preprocess(im)\n",
        "    images.append(image)\n",
        "    one_per_class.append(image)\n",
        "\n",
        "  descriptions = {}\n",
        "\n",
        "  for my_class in classes:\n",
        "    descriptions[my_class] = \"This is a image of \" + str(my_class)\n",
        "\n",
        "  texts = [descriptions[key] for key in descriptions]\n",
        "  image_input = torch.tensor(np.stack(one_per_class)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "\n",
        "  tokenizer = SimpleTokenizer()\n",
        "  text_tokens = [tokenizer.encode(desc) for desc in texts]\n",
        "\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "  eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      tokens = [sot_token] + tokens + [eot_token]\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  text_input = text_input.cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "\n",
        "  text_descriptions = list(descriptions.values())\n",
        "  image_input = torch.tensor(np.stack(images)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "\n",
        "  text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  text_input = text_input.cuda()\n",
        "  text_input.shape\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True) # 512 -> 256 -> 1 (1/0) (N -> 512)\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "      top_probs, top_labels = text_probs.cpu().topk(3, dim=-1)\n",
        "\n",
        "  plt.figure(figsize=(25, 60))\n",
        "  param_to_flask = []\n",
        "  for i, image in enumerate(images):\n",
        "      plt.subplot(17, 8, 2 * i + 1)\n",
        "      plt.imshow(image.permute(1, 2, 0))\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      plt.subplot(17, 8, 2 * i + 2)\n",
        "      y = np.arange(top_probs.shape[-1])\n",
        "      plt.grid()\n",
        "      plt.barh(y, top_probs[i])\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.gca().set_axisbelow(True)\n",
        "      plt.yticks(y, [text_descriptions[index].split(' ')[-1] for index in top_labels[i].numpy()])\n",
        "      param_to_flask.append([text_descriptions[index].split(' ')[-1] for index in top_labels[i].numpy()][0])\n",
        "\n",
        "  os.makedirs('/content/output_classes/',exist_ok=True)\n",
        "  for i in classes:\n",
        "    os.makedirs('/content/output_classes/'+i,exist_ok=True)\n",
        "  for i in range(0,len(list_sorted_of_photo)):\n",
        "    os.replace(\"/content/input_images/\"+list_sorted_of_photo[i], '/content/output_classes/'+str(param_to_flask[i])+'/'+list_sorted_of_photo[i])\n",
        "  \n",
        "  !zip -r /content/Result.zip /content/output_classes\n",
        "  dir = \"/content/output_classes/\"\n",
        "  dir = \"/content/output_classes/\"\n",
        "  shutil.rmtree(dir)\n",
        "  return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6-FJiZ8_phg"
      },
      "source": [
        "# Результаты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrLWnHMYIig6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "87b0119c-83c1-4cf8-a7a8-dde6d8ecaf03"
      },
      "source": [
        "!pip install flask_ngrok\n",
        "from flask import Flask, send_file\n",
        "from flask_ngrok import run_with_ngrok \n",
        "from flask import Flask, request, redirect, url_for \n",
        "from flask import send_from_directory \n",
        "from werkzeug.utils import secure_filename \n",
        "import io\n",
        "import zipfile\n",
        "from flask import Flask, request, send_file, make_response\n",
        "\n",
        "app = Flask(__name__) \n",
        "run_with_ngrok(app)    \n",
        " \n",
        "os.makedirs('input_images',exist_ok=True) \n",
        "UPLOAD_FOLDER = 'input_images' \n",
        "ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'bmp', 'NEF']) \n",
        " \n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER \n",
        " \n",
        " \n",
        "def allowed_file(filename): \n",
        "    return '.' in filename and filename.rsplit('.', 1)[1] in ALLOWED_EXTENSIONS \n",
        " \n",
        "@app.route('/', methods=['GET', 'POST']) \n",
        " \n",
        "def upload_file(): \n",
        "    if request.method == 'POST': \n",
        "        for file in request.files.getlist(\"file\"): \n",
        "            if file and allowed_file(file.filename): \n",
        "                filename = file.filename \n",
        "                file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename)) \n",
        "    return ''' \n",
        "    <!doctype html> \n",
        "    <html>\n",
        "    <script> \n",
        "    var intTextBox = 3;  \n",
        "    function addElement()  \n",
        "    { \n",
        "        intTextBox = intTextBox + 1; \n",
        "        var contentID = document.getElementById('content'); \n",
        "        var howManyTextBoxes = intTextBox;   \n",
        "        var newTBDiv = document.createElement('div');            \n",
        "        newTBDiv.setAttribute('id', 'strText' + intTextBox); \n",
        "        newTBDiv.innerHTML += `<p><input type=text name = class${intTextBox}>`;                              \n",
        "        contentID.appendChild(newTBDiv);    \n",
        "        return False                      \n",
        "    } \n",
        "    </script> \n",
        "    <head>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\">\n",
        "    </head>\n",
        "    <body>\n",
        "    <nav class=\"navbar navbar-light bg-light\">\n",
        "      <div class=\"container-fluid\">\n",
        "        <span class=\"navbar-brand mb-0 h1 \">4nn Task 3</span>\n",
        "      </div>\n",
        "    </nav>\n",
        "    <div class=\"container\" align=\"center\">\n",
        "      <title>4nn Team</title> \n",
        "      <h1>Upload new Files</h1> \n",
        "      <form action=\"\" method=post enctype=multipart/form-data> \n",
        "        <p><input type=file name=file multiple> \n",
        "          <input type=submit value=Upload> \n",
        "      </form>\n",
        "    </div>\n",
        "    <div class=\"container\" align=\"center\">\n",
        "      <h3>Custom classes</h3> \n",
        "        <form id=content action=\"sort\" method=post>\n",
        "          <p><input type=text name = class1> \n",
        "          <p><input type=text name = class2>\n",
        "          <p><input type=text name = class3 style = \"margin-left: 10.5%;\">\n",
        "          <a href=\"javascript:addElement();\"><input type=\"button\"  value=\"Add class\"></a> \n",
        "          <input type=submit value=Sort>\n",
        "        </form>\n",
        "    </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    ''' \n",
        " \n",
        " \n",
        "@app.route('/sort/', methods=['GET', 'POST']) \n",
        "def make_sorted_arhiv(): \n",
        "    if request.method == 'POST': \n",
        "        classes = [] \n",
        "        for key in request.form: \n",
        "            id_ = key.partition('.')[-1] \n",
        "            classes.append(request.form[key]) \n",
        "        print(classes)\n",
        "\n",
        "        try:\n",
        "          os.remove(\"/content/Result.zip\")\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "        main_processing(classes)\n",
        "\n",
        "        app.config['UPLOAD_FOLDER'] = \".\" \n",
        "        return send_from_directory(app.config['UPLOAD_FOLDER'], \"Result.zip\", as_attachment=True)\n",
        " \n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-6-6807a27dde44>\", line 3, in <module>\n",
            "    from flask_ngrok import run_with_ngrok\n",
            "ModuleNotFoundError: No module named 'flask_ngrok'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}