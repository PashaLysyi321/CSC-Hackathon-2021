{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ImageSortNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PashaLysyi321/CSC-Hackathon-2021/blob/main/ImageSortNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "07e5ae85-9fec-466a-ce40-2283d335412c"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "# ! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n",
            "Torch version: 1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Загружаем CLIP\n",
        "\n",
        "Скачиваем CLIP, предобученный на 400М пар изображение-текст.  Его можно использовать в режиме обучения без обучения (например ViT-B/32 CLIP). После запуска блока нас ждет установка скачивание model.pt модели CLIP: Visual Transformer \"ViT-B/32\" + Text Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTJ3CrSIw2fe",
        "outputId": "cdd6548b-6370-4f8b-948e-27f4646c7487"
      },
      "source": [
        "MODELS = {\n",
        "    \"ViT-B/32\":  \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "}\n",
        "\n",
        "! wget {MODELS[\"ViT-B/32\"]} -O model.pt\n",
        "\n",
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "\n",
        "\n",
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-05 13:50:42--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.71, 13.107.213.71, 2620:1ec:bdf::71, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353976522 (338M) [application/octet-stream]\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>] 337.58M  36.6MB/s    in 6.0s    \n",
            "\n",
            "2021-07-05 13:50:48 (56.1 MB/s) - ‘model.pt’ saved [353976522/353976522]\n",
            "\n",
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "--2021-07-05 13:50:51--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.71, 13.107.213.71, 2620:1ec:bdf::71, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-07-05 13:50:51 (23.6 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Препроцессинг текста и изображений\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-erzoVgzoT4"
      },
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import os\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import shutil\n",
        "\n",
        "def main_processing(classes):\n",
        "  images = []\n",
        "  one_per_class = []\n",
        "\n",
        "  per_class_lists = []\n",
        "\n",
        "  list_sorted_of_photo = sorted(os.listdir(path=\"/content/input_images/\"))\n",
        "  for i in list_sorted_of_photo:\n",
        "    im = Image.open(\"/content/input_images/\"+i).convert('RGB')\n",
        "    image = preprocess(im)\n",
        "    if image.shape[0] != 3:\n",
        "        raise TypeError(\"Only 3-channel RGB image are allowed\")\n",
        "    images.append(image)\n",
        "    one_per_class.append(image)\n",
        "\n",
        "  descriptions = {}\n",
        "\n",
        "  for my_class in classes:\n",
        "    descriptions[my_class] = \"This is a image of \" + str(my_class)\n",
        "\n",
        "  texts = [descriptions[key] for key in descriptions]\n",
        "  image_input = torch.tensor(np.stack(one_per_class)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "\n",
        "  tokenizer = SimpleTokenizer()\n",
        "  text_tokens = [tokenizer.encode(desc) for desc in texts]\n",
        "\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "  eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      tokens = [sot_token] + tokens + [eot_token]\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  text_input = text_input.cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "\n",
        "  text_descriptions = list(descriptions.values())\n",
        "  image_input = torch.tensor(np.stack(images)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "\n",
        "  text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  text_input = text_input.cuda()\n",
        "  text_input.shape\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True) # 512 -> 256 -> 1 (1/0) (N -> 512)\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "      top_probs, top_labels = text_probs.cpu().topk(3, dim=-1)\n",
        "\n",
        "  plt.figure(figsize=(25, 60))\n",
        "  param_to_flask = []\n",
        "  for i, image in enumerate(images):\n",
        "      plt.subplot(17, 8, 2 * i + 1)\n",
        "      plt.imshow(image.permute(1, 2, 0))\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      plt.subplot(17, 8, 2 * i + 2)\n",
        "      y = np.arange(top_probs.shape[-1])\n",
        "      plt.grid()\n",
        "      plt.barh(y, top_probs[i])\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.gca().set_axisbelow(True)\n",
        "      plt.yticks(y, [text_descriptions[index].split(' ')[-1] for index in top_labels[i].numpy()])\n",
        "      param_to_flask.append([text_descriptions[index].split(' ')[-1] for index in top_labels[i].numpy()][0])\n",
        "\n",
        "  os.makedirs('/content/output_classes/',exist_ok=True)\n",
        "  for i in classes:\n",
        "    os.makedirs('/content/output_classes/'+i,exist_ok=True)\n",
        "  for i in range(0,len(list_sorted_of_photo)):\n",
        "    os.replace(\"/content/input_images/\"+list_sorted_of_photo[i], '/content/output_classes/'+str(param_to_flask[i])+'/'+list_sorted_of_photo[i])\n",
        "  \n",
        "  !zip -r /content/Result.zip /content/output_classes\n",
        "  dir = \"/content/output_classes/\"\n",
        "  dir = \"/content/output_classes/\"\n",
        "  shutil.rmtree(dir)\n",
        "  return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6-FJiZ8_phg"
      },
      "source": [
        "# WEB сервис для сортировки изображений по выбраным пользователем классам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrLWnHMYIig6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a16cbb4-b97b-43ee-b546-27ad9f4347c7"
      },
      "source": [
        "!pip install flask_ngrok\n",
        "from flask import Flask, send_file\n",
        "from flask_ngrok import run_with_ngrok \n",
        "from flask import Flask, request, redirect, url_for \n",
        "from flask import send_from_directory \n",
        "from werkzeug.utils import secure_filename \n",
        "import io\n",
        "import zipfile\n",
        "from flask import Flask, request, send_file, make_response\n",
        "\n",
        "app = Flask(__name__) \n",
        "run_with_ngrok(app)    \n",
        " \n",
        "os.makedirs('input_images',exist_ok=True) \n",
        "UPLOAD_FOLDER = 'input_images' \n",
        "ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'bmp', 'NEF']) \n",
        " \n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER \n",
        " \n",
        " \n",
        "def allowed_file(filename): \n",
        "    return '.' in filename and filename.rsplit('.', 1)[1] in ALLOWED_EXTENSIONS \n",
        " \n",
        "@app.route('/', methods=['GET', 'POST']) \n",
        " \n",
        "def upload_file(): \n",
        "    if request.method == 'POST': \n",
        "        for file in request.files.getlist(\"file\"): \n",
        "            if file and allowed_file(file.filename): \n",
        "                filename = file.filename \n",
        "                file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename)) \n",
        "    return ''' \n",
        "    <!doctype html> \n",
        "    <html>\n",
        "    <script> \n",
        "    var intTextBox = 3;  \n",
        "    function addElement()  \n",
        "    { \n",
        "        intTextBox = intTextBox + 1; \n",
        "        var contentID = document.getElementById('content'); \n",
        "        var howManyTextBoxes = intTextBox;   \n",
        "        var newTBDiv = document.createElement('div');            \n",
        "        newTBDiv.setAttribute('id', 'strText' + intTextBox); \n",
        "        newTBDiv.innerHTML += `<p><input type=text name = class${intTextBox}>`;                              \n",
        "        contentID.appendChild(newTBDiv);    \n",
        "        return False                      \n",
        "    } \n",
        "    </script> \n",
        "    <head>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\">\n",
        "    </head>\n",
        "    <body>\n",
        "    <nav class=\"navbar navbar-light bg-light\">\n",
        "      <div class=\"container-fluid\">\n",
        "        <span class=\"navbar-brand mb-0 h1 \">4nn Task 3</span>\n",
        "      </div>\n",
        "    </nav>\n",
        "    <div class=\"container\" align=\"center\">\n",
        "      <title>4nn Team</title> \n",
        "      <h1>Upload new Files</h1> \n",
        "      <form action=\"\" method=post enctype=multipart/form-data> \n",
        "        <p><input type=file name=file multiple> \n",
        "          <input type=submit value=Upload> \n",
        "      </form>\n",
        "    </div>\n",
        "    <div class=\"container\" align=\"center\">\n",
        "      <h3>Custom classes</h3> \n",
        "        <form id=content action=\"sort\" method=post>\n",
        "          <p><input type=text name = class1> \n",
        "          <p><input type=text name = class2>\n",
        "          <p><input type=text name = class3 style = \"margin-left: 10.5%;\">\n",
        "          <a href=\"javascript:addElement();\"><input type=\"button\"  value=\"Add class\"></a> \n",
        "          <input type=submit value=Sort>\n",
        "        </form>\n",
        "    </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    ''' \n",
        " \n",
        "@app.route('/sort/', methods=['GET', 'POST']) \n",
        "def make_sorted_arhiv(): \n",
        "    if request.method == 'POST': \n",
        "        classes = [] \n",
        "        for key in request.form: \n",
        "            id_ = key.partition('.')[-1] \n",
        "            classes.append(request.form[key]) \n",
        "        print(classes)\n",
        "\n",
        "        try:\n",
        "          os.remove(\"/content/Result.zip\")\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "        main_processing(classes)\n",
        "\n",
        "        app.config['UPLOAD_FOLDER'] = \".\" \n",
        "        return send_from_directory(app.config['UPLOAD_FOLDER'], \"Result.zip\", as_attachment=True)\n",
        " \n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2021.5.30)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://44e1a63ae11c.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}